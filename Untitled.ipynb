{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6bbbea-acf3-4174-8eac-dc370184113f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Live Predictions ---\n",
      "Skipping COIN: Not enough clean data for training (0 rows).\n",
      "Skipping TSLA: Not enough clean data for training (0 rows).\n",
      "Skipping GOOGL: Not enough clean data for training (0 rows).\n",
      "Skipping NVDA: Not enough clean data for training (0 rows).\n",
      "Skipping AAPL: Not enough clean data for training (0 rows).\n",
      "Skipping NKE: Not enough clean data for training (0 rows).\n",
      "Skipping SMCI: Not enough clean data for training (0 rows).\n",
      "Skipping XPEV: Not enough clean data for training (0 rows).\n",
      "Skipping NIO: Not enough clean data for training (0 rows).\n",
      "Skipping UNH: Not enough clean data for training (0 rows).\n",
      "Skipping XYZ: Not enough clean data for training (0 rows).\n",
      "\n",
      "=== Live Predictions Summary ===\n",
      "No live predictions generated.\n",
      "\n",
      "--- Running Backtests ---\n",
      "Starting backtest for COIN...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Return'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Return'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 813\u001b[39m\n\u001b[32m    809\u001b[39m all_stats = []\n\u001b[32m    811\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m TICKERS:\n\u001b[32m    812\u001b[39m     \u001b[38;5;66;03m# Use the revised backtesting function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m     trades, stats = \u001b[43mtrain_and_backtest_revised\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_train_years\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbacktest_days\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m252\u001b[39;49m\u001b[43m*\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_every_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# show every 10th trade on plot\u001b[39;00m\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trades.empty:\n\u001b[32m    816\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtick\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has no trades after filtering in backtest.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 557\u001b[39m, in \u001b[36mtrain_and_backtest_revised\u001b[39m\u001b[34m(ticker, initial_train_years, backtest_days, show_every_n)\u001b[39m\n\u001b[32m    555\u001b[39m signals_df = pd.DataFrame(signals).set_index(\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    556\u001b[39m trades = signals_df[signals_df[\u001b[33m'\u001b[39m\u001b[33mSignal_Generated\u001b[39m\u001b[33m'\u001b[39m]].copy()\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m trades = trades[\u001b[43mtrades\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mReturn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.notna()] \u001b[38;5;66;03m# Ensure return is calculated\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trades) == \u001b[32m0\u001b[39m:\n\u001b[32m    560\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] No valid trades generated after backtesting and filtering.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Return'"
     ]
    }
   ],
   "source": [
    "import ta_functions as ta\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBRegressor # Keep if you plan to use it, currently unused\n",
    "from sklearn.pipeline import Pipeline # Keep if you plan to use it, currently unused\n",
    "\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Global Configurations ---\n",
    "TICKERS = [\"COIN\", \"TSLA\", \"GOOGL\", \"NVDA\", \"AAPL\", \"NKE\", \"SMCI\", \"XPEV\", \"NIO\", \"UNH\", \"XYZ\"]\n",
    "YEARS_OF_DATA = 2 # For initial data fetch for live predictions\n",
    "PROFIT_TARGET = 0.05\n",
    "STOP_LOSS = 0.04\n",
    "FORWARD_DAYS = 14 # For prediction window\n",
    "future_window = 14 # For backtesting trade evaluation\n",
    "\n",
    "tolerance = 1.07 # Allows entry if current price is within 7% of predicted entry\n",
    "\n",
    "# Time window for current predictions (latest data)\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=365 * YEARS_OF_DATA)\n",
    "\n",
    "# Shared model components\n",
    "FEATURES = ['RSI', 'RSI_SMA', 'SMA1', 'SMA2', 'SMA3',\n",
    "            'SMA_Ratio', 'MACD', 'Signal_Line', 'Upper_Band', 'Lower_Band', 'Volume_MA20',\n",
    "            '5_day_return', '10_day_return', 'Volatility', 'CCI', 'OBV', '+DI',\n",
    "            '-DI', 'ADX', 'ATR', 'VWMA', 'VI+','VI-', 'KCu','KCl', 'STu', 'STl', 'Candlesticks', # Changed 'VI+' to 'VI-' to match calculation\n",
    "            'Bear', 'Bull', 'vSpike', 'DD'] # Added 'Candlesticks' from add_technical_indicators\n",
    "\n",
    "results = [] # To store live prediction results\n",
    "\n",
    "# --- Data Acquisition and Feature Engineering Functions ---\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    \"\"\"Downloads stock data from Yahoo Finance.\"\"\"\n",
    "    df = yf.download(ticker, start=start_date, end=end_date + timedelta(days=1), # +1 day to ensure end_date is included\n",
    "                     interval='1d', auto_adjust=False, progress=False)\n",
    "    df = df.reset_index()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns] # Handle multi-level columns\n",
    "    df = df.dropna() # Drop rows with any NaN after initial download\n",
    "    return df\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    \"\"\"Adds various technical indicators to the DataFrame.\"\"\"\n",
    "    # Ensure all necessary columns exist before calculations\n",
    "    if not all(col in df.columns for col in ['Open', 'High', 'Low', 'Close', 'Volume']):\n",
    "        raise ValueError(\"DataFrame must contain 'Open', 'High', 'Low', 'Close', 'Volume' columns.\")\n",
    "\n",
    "    df['SMA1'] = df['Close'].rolling(window=12).mean()\n",
    "    df['SMA2'] = df['Close'].rolling(window=24).mean()\n",
    "    df['SMA3'] = df['Close'].rolling(window=52).mean()\n",
    "    df['SMA_Ratio'] = df['SMA1'] / df['SMA2']\n",
    "    \n",
    "    df['Bear'] = (df['SMA1'] < df['SMA2']).astype(int)\n",
    "    df['Bull'] = (df['SMA2'] < df['SMA1']).astype(int)\n",
    "    \n",
    "    df['RSI'] = ta.calculate_rsi(df)\n",
    "    df['RSI_SMA'] = df['RSI'] / df['RSI'].rolling(14).mean() # Corrected to avoid division by zero if RSI_SMA is used as a feature\n",
    "    \n",
    "    ema12 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df['Close'].ewm(span=24, adjust=False).mean()\n",
    "    df['MACD'] = ema12 - ema26\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    df['Upper_Band'] = df['SMA1'] + (2 * df['Close'].rolling(20).std())\n",
    "    df['Lower_Band'] = df['SMA1'] - (2 * df['Close'].rolling(20).std())\n",
    "    \n",
    "    df['Volume_MA20'] = df['Volume'].rolling(window=20).mean()\n",
    "    df['vSpike'] = (df['Volume'] > 2 * df['Volume_MA20']).astype(int)\n",
    "    \n",
    "    df['CCI'] = ta.calculate_cci(df)\n",
    "    df['OBV'] = ta.calculate_obv(df)\n",
    "    \n",
    "    dmi_results = ta.calculate_dmi(df, n=14)\n",
    "    if dmi_results is not None and len(dmi_results) == 3: # Ensure 3 columns are returned\n",
    "        df[['+DI', '-DI', 'ADX']] = dmi_results\n",
    "    else:\n",
    "        # Handle cases where dmi might return unexpected output or NaNs\n",
    "        df[['+DI', '-DI', 'ADX']] = np.nan, np.nan, np.nan # Fill with NaN if not calculated\n",
    "        \n",
    "    df['ATR'] = ta.calculate_atr(high=df.High, low=df.Low, close=df.Close)\n",
    "    \n",
    "    df['VWMA'] = ta.calculate_vwma(df)\n",
    "    \n",
    "    keltner_results = ta.calculate_keltner(df)\n",
    "    if keltner_results is not None and len(keltner_results) == 3:\n",
    "        df[['KCm', 'KCu', 'KCl']] = keltner_results\n",
    "    else:\n",
    "        df[['KCm', 'KCu', 'KCl']] = np.nan, np.nan, np.nan\n",
    "        \n",
    "    vortex_results = ta.calculate_vortex(df)\n",
    "    if vortex_results is not None and len(vortex_results) == 2:\n",
    "        df[['VI+', 'VI-']] = vortex_results\n",
    "    else:\n",
    "        df[['VI+', 'VI-']] = np.nan, np.nan\n",
    "        \n",
    "    supertrend_results = ta.calculate_supertrend(df)\n",
    "    if supertrend_results is not None and len(supertrend_results) == 2:\n",
    "        df[['STu', 'STl']] = supertrend_results\n",
    "    else:\n",
    "        df[['STu', 'STl']] = np.nan, np.nan\n",
    "    \n",
    "    # Add Candlestick Patterns\n",
    "    df = ta.add_candlestickpatterns(df) # This function usually adds columns like 'Candlesticks'\n",
    "    if 'Candlesticks' not in df.columns: # Ensure 'Candlesticks' column is created\n",
    "        df['Candlesticks'] = 0 # Default to 0 if no patterns found or function fails\n",
    "\n",
    "    df['DD'] = df['Close'].where(df['Close'] < df['Close'].shift(1)).std() # Daily Drawdown std\n",
    "\n",
    "    df['5_day_return'] = df['Close'].pct_change(5)\n",
    "    df['10_day_return'] = df['Close'].pct_change(10)\n",
    "    df['Volatility'] = df['Close'].rolling(14).std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Target Variable Calculation Functions (Look-ahead for training, not for prediction features) ---\n",
    "def compute_expected_return(df, forward_days=FORWARD_DAYS):\n",
    "    \"\"\"Calculates the maximum future return within a forward_days window.\"\"\"\n",
    "    df['Expected_Return'] = np.nan\n",
    "    close_prices = df['Close'].values\n",
    "    for i in range(len(close_prices) - forward_days):\n",
    "        current_price = close_prices[i]\n",
    "        future_max = np.nanmax(close_prices[i + 1:i + 1 + forward_days])\n",
    "        expected_return = (future_max - current_price) / current_price\n",
    "        df.iloc[i, df.columns.get_loc('Expected_Return')] = expected_return\n",
    "    return df\n",
    "\n",
    "def compute_expected_loss(df, forward_days=FORWARD_DAYS):\n",
    "    \"\"\"Calculates the minimum future return (max loss) within a forward_days window.\"\"\"\n",
    "    df['Expected_Loss'] = np.nan\n",
    "    close_prices = df['Close'].values\n",
    "    for i in range(len(close_prices) - forward_days):\n",
    "        current_price = close_prices[i]\n",
    "        future_min = np.nanmin(close_prices[i + 1:i + 1 + forward_days])\n",
    "        expected_loss = (future_min - current_price) / current_price\n",
    "        df.iloc[i, df.columns.get_loc('Expected_Loss')] = expected_loss\n",
    "    return df\n",
    "\n",
    "def compute_expected_entry(df, n=3):\n",
    "    \"\"\"\n",
    "    Predicts a potential optimal entry price, e.g., the minimum low in the next 'n' days.\n",
    "    This is for training the entry model.\n",
    "    \"\"\"\n",
    "    # Shift(-n) looks 'n' days into the future for the current row\n",
    "    df['Expected_Entry'] = df['Low'].rolling(window=n, min_periods=1).min().shift(-(n-1)) # shifted by -(n-1) to align with actual next day, e.g., n=3, shift(-2)\n",
    "    return df\n",
    "\n",
    "def label_tp_hit(df, window=FORWARD_DAYS, profit_target=PROFIT_TARGET, stop_loss=STOP_LOSS):\n",
    "    \"\"\"\n",
    "    Labels each row:\n",
    "    1 = TP hit before SL\n",
    "    0 = SL hit before TP or neither hit (or if TP is hit but SL is hit earlier or simultaneously)\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    close_prices = df['Close'].values\n",
    "    high_prices = df['High'].values # Use High for TP check\n",
    "    low_prices = df['Low'].values   # Use Low for SL check\n",
    "\n",
    "    for i in range(len(close_prices) - window):\n",
    "        current_price = close_prices[i]\n",
    "        tp_level = current_price * (1 + profit_target)\n",
    "        sl_level = current_price * (1 - stop_loss) # Stop loss is a positive value, so 1 - SL\n",
    "\n",
    "        # Future window for evaluation\n",
    "        future_highs = high_prices[i + 1:i + 1 + window]\n",
    "        future_lows = low_prices[i + 1:i + 1 + window]\n",
    "\n",
    "        tp_hit_day = next((j for j, price in enumerate(future_highs) if price >= tp_level), None)\n",
    "        sl_hit_day = next((j for j, price in enumerate(future_lows) if price <= sl_level), None)\n",
    "\n",
    "        if tp_hit_day is not None and (sl_hit_day is None or tp_hit_day < sl_hit_day):\n",
    "            labels.append(1) # TP hit before or without SL\n",
    "        else:\n",
    "            labels.append(0) # SL hit before TP, or neither hit, or SL hit first\n",
    "\n",
    "    labels += [np.nan] * window\n",
    "    df['TP_Hit_Label'] = labels\n",
    "    return df\n",
    "\n",
    "# --- Live Prediction Function ---\n",
    "def make_live_predictions(tickers, start_date, end_date, features):\n",
    "    \"\"\"\n",
    "    Generates live predictions for a list of tickers using models trained on\n",
    "    the entire available historical data.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            df = get_stock_data(ticker, start_date, end_date)\n",
    "            if df.empty:\n",
    "                print(f\"Skipping {ticker}: No data downloaded.\")\n",
    "                continue\n",
    "\n",
    "            df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')\n",
    "            df = add_technical_indicators(df)\n",
    "            \n",
    "            # These are for training targets, we calculate them on the full dataset\n",
    "            # up to the second to last day. The last day is for actual prediction.\n",
    "            df_for_training = df.iloc[:-1].copy() # Exclude the very last day for target calculation\n",
    "\n",
    "            df_for_training = compute_expected_return(df_for_training, FORWARD_DAYS)\n",
    "            df_for_training = compute_expected_loss(df_for_training, FORWARD_DAYS)\n",
    "            df_for_training = compute_expected_entry(df_for_training, 3) # Using n=3 for entry prediction\n",
    "\n",
    "            # Label for classification model\n",
    "            df_for_training = label_tp_hit(df_for_training, FORWARD_DAYS, PROFIT_TARGET, STOP_LOSS)\n",
    "\n",
    "            # Prepare data for model training\n",
    "            # We need to ensure FEATURES are available and valid in the training data\n",
    "            required_features_and_labels = features + ['Expected_Return', 'Expected_Loss', 'Expected_Entry', 'TP_Hit_Label']\n",
    "            df_model = df_for_training.dropna(subset=required_features_and_labels)\n",
    "            \n",
    "            if len(df_model) < 20:\n",
    "                print(f\"Skipping {ticker}: Not enough clean data for training ({len(df_model)} rows).\")\n",
    "                continue\n",
    "\n",
    "            X = df_model[features]\n",
    "            \n",
    "            # --- Scalers for each model ---\n",
    "            scaler_return = StandardScaler()\n",
    "            scaler_loss = StandardScaler()\n",
    "            scaler_entry = StandardScaler()\n",
    "            scaler_class = StandardScaler()\n",
    "\n",
    "            X_scaled_return = scaler_return.fit_transform(X)\n",
    "            X_scaled_loss = scaler_loss.fit_transform(X)\n",
    "            X_scaled_entry = scaler_entry.fit_transform(X)\n",
    "            X_scaled_class = scaler_class.fit_transform(X) # Can reuse scaler_return if features are the same\n",
    "\n",
    "            # --- Train Return Model ---\n",
    "            y_return = df_model['Expected_Return']\n",
    "            model_return = RandomForestRegressor(\n",
    "                n_estimators=200, max_depth=10, min_samples_leaf=5, max_features='sqrt', ccp_alpha=0.01, random_state=42\n",
    "            )\n",
    "            model_return.fit(X_scaled_return, y_return)\n",
    "\n",
    "            # --- Train Loss Model ---\n",
    "            y_loss = df_model['Expected_Loss']\n",
    "            model_loss = RandomForestRegressor(\n",
    "                n_estimators=200, max_depth=10, min_samples_leaf=5, max_features='sqrt', ccp_alpha=0.01, random_state=42\n",
    "            )\n",
    "            model_loss.fit(X_scaled_loss, y_loss)\n",
    "\n",
    "            # --- Train Entry Model ---\n",
    "            y_entry = df_model['Expected_Entry']\n",
    "            model_entry = RandomForestRegressor(\n",
    "                n_estimators=200, max_depth=10, min_samples_leaf=5, max_features='sqrt', ccp_alpha=0.01, random_state=42\n",
    "            )\n",
    "            model_entry.fit(X_scaled_entry, y_entry)\n",
    "\n",
    "            # --- Train Classification Model ---\n",
    "            y_class = df_model['TP_Hit_Label'].astype(int)\n",
    "            model_class = RandomForestClassifier(\n",
    "                n_estimators=200, max_depth=10, min_samples_leaf=5, random_state=42\n",
    "            )\n",
    "            model_class.fit(X_scaled_class, y_class)\n",
    "\n",
    "            # --- Make Predictions for the latest day ---\n",
    "            latest = df.iloc[[-1]] # The actual latest row of the original df\n",
    "            if latest[features].isnull().values.any():\n",
    "                print(f\"Skipping {ticker}: Latest data point has missing features.\")\n",
    "                continue\n",
    "            \n",
    "            # Scale latest features for prediction\n",
    "            latest_scaled_return = scaler_return.transform(latest[features])\n",
    "            latest_scaled_loss = scaler_loss.transform(latest[features])\n",
    "            latest_scaled_entry = scaler_entry.transform(latest[features])\n",
    "            latest_scaled_class = scaler_class.transform(latest[features])\n",
    "            \n",
    "            predicted_return = model_return.predict(latest_scaled_return)[0]\n",
    "            predicted_loss = model_loss.predict(latest_scaled_loss)[0]\n",
    "            predicted_entry = model_entry.predict(latest_scaled_entry)[0]\n",
    "            \n",
    "            tp_hit_prob = model_class.predict_proba(latest_scaled_class)[0][1] # Probability of TP hit\n",
    "\n",
    "            current_price = latest['Close'].values[0]\n",
    "            \n",
    "            # Determine the 'best_entry' as the predicted entry\n",
    "            best_entry = predicted_entry\n",
    "            \n",
    "            # Adjust TP/SL based on this best_entry\n",
    "            predicted_tp = best_entry * (1 + predicted_return)\n",
    "            \n",
    "            # Ensure predicted_loss is negative for SL calculation\n",
    "            # Use min(predicted_loss, -STOP_LOSS) to ensure the predicted loss is at least the predefined STOP_LOSS\n",
    "            predicted_sl_value = predicted_loss # raw prediction from model\n",
    "            final_sl_factor = min(predicted_sl_value, -STOP_LOSS) # Use the more conservative (larger absolute) stop loss\n",
    "            predicted_sl = best_entry * (1 + final_sl_factor) # predicted_loss from model is already negative\n",
    "            \n",
    "            entry_discount_pct = ((best_entry - current_price) / current_price) * 100\n",
    "\n",
    "            # Confidence score calculation\n",
    "            # Avoid division by zero and handle cases where predicted_loss might be positive\n",
    "            if predicted_loss < 0: # Ensure loss is indeed a loss (negative)\n",
    "                rr = predicted_return / abs(predicted_loss)\n",
    "            else:\n",
    "                rr = 0 # No risk (or positive loss, which is wrong), so RR is zero or undefined\n",
    "            \n",
    "            confidence_score = tp_hit_prob * max(rr, 0) # Only consider positive risk-reward\n",
    "\n",
    "            # Technical condition check for \"Signal\"\n",
    "            sma1 = latest['SMA1'].values[0]\n",
    "            sma2 = latest['SMA2'].values[0]\n",
    "            rsi = latest['RSI'].values[0]\n",
    "\n",
    "            signal = \"âš ï¸ Neutral\"\n",
    "            # Using current_price relative to SMA for signal\n",
    "            if (current_price > sma1 and sma1 > sma2 and rsi > 52):\n",
    "                signal = \"âœ… Bullish\"\n",
    "            elif (current_price < sma1 and sma1 < sma2 and rsi < 48):\n",
    "                 signal = \"ðŸ”» Bearish\"\n",
    "            \n",
    "            # Risk assessment\n",
    "            risk_label = \"ðŸŸ¢ Low Risk\" if abs(predicted_loss) <= STOP_LOSS else \"ðŸ”´ High Risk\"\n",
    "\n",
    "            all_results.append({\n",
    "                \"Ticker\": ticker,\n",
    "                \"Date\": latest.index[-1].date(),\n",
    "                \"Current_Price\": round(current_price, 2), # Changed from \"Price\" to \"Current_Price\"\n",
    "                \"Predicted_Entry\": round(best_entry, 2), # Renamed \"Entry\" to \"Predicted_Entry\"\n",
    "                \"Entry_Diff_Pct\": round(entry_discount_pct, 2), # Renamed \"Entry%\"\n",
    "                \"Predicted_Max_Return_Pct\": round(predicted_return * 100, 2), # Renamed \"Max (%)\"\n",
    "                \"Predicted_TP_Price\": round(predicted_tp, 2), # Renamed \"TP\"\n",
    "                \"Predicted_SL_Price\": round(predicted_sl, 2), # Renamed \"SL\"\n",
    "                \"Predicted_Max_Loss_Pct\": round(predicted_loss * 100, 2), # Renamed \"Loss (%)\"\n",
    "                \"Technical_Signal\": signal, # Renamed \"Signal\"\n",
    "                \"Risk_Assessment\": risk_label, # Renamed \"Risk\"\n",
    "                \"TP_Hit_Probability\": round(tp_hit_prob * 100, 2), # Renamed \"TP_Prob\"\n",
    "                \"Confidence_Score\": round(confidence_score * 100, 2), # Renamed \"Confidence\"\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker} for live prediction: {e}\")\n",
    "            \n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# --- Backtesting Function (Revised for Rolling Window) ---\n",
    "def train_and_backtest_revised(ticker=\"\", initial_train_years=2, backtest_days=252, show_every_n=1):\n",
    "    \"\"\"\n",
    "    Performs a rolling window backtest for a given ticker.\n",
    "    initial_train_years: Number of years for the initial training set.\n",
    "    backtest_days: Number of trading days to backtest (approx 252 days per year).\n",
    "    show_every_n: For plotting, shows every Nth trade annotation to avoid clutter.\n",
    "    \"\"\"\n",
    "    print(f\"Starting backtest for {ticker}...\")\n",
    "    \n",
    "    # 1. Get and prepare full data\n",
    "    # Download more data than strictly needed to ensure enough training history\n",
    "    end_date_full = datetime.now()\n",
    "    # Ensure enough historical data for initial training and indicator calculation\n",
    "    start_date_full = end_date_full - timedelta(days=365 * (initial_train_years + (backtest_days / 365) + 0.5)) # Added 0.5 years buffer\n",
    "    \n",
    "    df_full = get_stock_data(ticker, start_date_full, end_date_full)\n",
    "    if df_full.empty:\n",
    "        print(f\"[{ticker}] No data downloaded for backtest.\")\n",
    "        return pd.DataFrame(), {}\n",
    "    \n",
    "    df_full['Volume'] = pd.to_numeric(df_full['Volume'], errors='coerce')\n",
    "    df_full = add_technical_indicators(df_full)\n",
    "    \n",
    "    signals = [] # To store all generated signals (including no trades)\n",
    "\n",
    "    # Determine the actual start of the backtesting period\n",
    "    # Ensure there's at least initial_train_years worth of data before backtest_start_date\n",
    "    min_data_points_for_initial_train = int(365 * initial_train_years) + 52 # approx days for 2 years + longest SMA window\n",
    "    \n",
    "    if len(df_full) <= min_data_points_for_initial_train + FORWARD_DAYS + 1: # +1 for current day, +FORWARD_DAYS for future window\n",
    "        print(f\"[{ticker}] Not enough data for backtest. Requires at least {min_data_points_for_initial_train + FORWARD_DAYS + 1} data points.\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "    # Iterate day by day through the backtesting period\n",
    "    # The loop starts from where enough training data is available\n",
    "    start_idx_for_backtest_loop = min_data_points_for_initial_train \n",
    "    \n",
    "    for i in range(start_idx_for_backtest_loop, len(df_full) - FORWARD_DAYS): # Loop up to last day minus FORWARD_DAYS to allow future window\n",
    "        current_date_idx = i\n",
    "        \n",
    "        # Define the training window (all data *before* current_date_idx)\n",
    "        train_window_df = df_full.iloc[:current_date_idx].copy()\n",
    "        \n",
    "        # Define the current day for prediction\n",
    "        current_day_df = df_full.iloc[current_date_idx:current_date_idx+1].copy()\n",
    "        \n",
    "        if current_day_df.empty or current_day_df[FEATURES].isnull().values.any():\n",
    "            # If current day's features are incomplete, skip this day\n",
    "            signals.append({\n",
    "                'Date': df_full.index[current_date_idx], # Still record the date\n",
    "                'Signal_Generated': False,\n",
    "                'Result': \"Skipped (Missing Features)\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Compute future-based labels for the TRAINING DATA ONLY\n",
    "        train_window_df = compute_expected_return(train_window_df, FORWARD_DAYS)\n",
    "        train_window_df = compute_expected_loss(train_window_df, FORWARD_DAYS)\n",
    "        train_window_df = compute_expected_entry(train_window_df, 3) # Use n=3 for entry\n",
    "\n",
    "        # Label for classification model\n",
    "        train_window_df = label_tp_hit(train_window_df, FORWARD_DAYS, PROFIT_TARGET, STOP_LOSS)\n",
    "\n",
    "        # Drop NaNs after calculating all features and labels for the training window\n",
    "        required_train_cols = FEATURES + ['Expected_Return', 'Expected_Loss', 'Expected_Entry', 'TP_Hit_Label']\n",
    "        train_window_df_cleaned = train_window_df.dropna(subset=required_train_cols)\n",
    "\n",
    "        if len(train_window_df_cleaned) < 50: # Ensure sufficient training data\n",
    "            print(f\"[{ticker}] Not enough clean training data at {df_full.index[current_date_idx].date()}. ({len(train_window_df_cleaned)} rows). Skipping.\")\n",
    "            signals.append({\n",
    "                'Date': df_full.index[current_date_idx],\n",
    "                'Signal_Generated': False,\n",
    "                'Result': \"Skipped (Not Enough Training Data)\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Define features and targets for training\n",
    "        X_train = train_window_df_cleaned[FEATURES]\n",
    "        y_return = train_window_df_cleaned['Expected_Return']\n",
    "        y_loss = train_window_df_cleaned['Expected_Loss']\n",
    "        y_entry = train_window_df_cleaned['Expected_Entry']\n",
    "        y_class = train_window_df_cleaned['TP_Hit_Label'].astype(int)\n",
    "\n",
    "        # Initialize and train models\n",
    "        # Using a Pipeline with StandardScaler for consistent scaling within the loop\n",
    "        pipeline_return = Pipeline([('scaler', StandardScaler()), ('regressor', RandomForestRegressor(n_estimators=200, max_depth=7, min_samples_leaf=5, max_features='sqrt', ccp_alpha=0.01, random_state=42))])\n",
    "        pipeline_loss = Pipeline([('scaler', StandardScaler()), ('regressor', RandomForestRegressor(n_estimators=200, max_depth=7, min_samples_leaf=5, max_features='sqrt', ccp_alpha=0.01, random_state=42))])\n",
    "        pipeline_entry = Pipeline([('scaler', StandardScaler()), ('regressor', RandomForestRegressor(n_estimators=200, max_depth=7, min_samples_leaf=5, max_features='sqrt', ccp_alpha=0.01, random_state=42))])\n",
    "        pipeline_class = Pipeline([('scaler', StandardScaler()), ('classifier', RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_leaf=5, random_state=42))])\n",
    "\n",
    "        pipeline_return.fit(X_train, y_return)\n",
    "        pipeline_loss.fit(X_train, y_loss)\n",
    "        pipeline_entry.fit(X_train, y_entry)\n",
    "        pipeline_class.fit(X_train, y_class)\n",
    "\n",
    "        # Predict for the current day (using only its features)\n",
    "        pred_return = pipeline_return.predict(current_day_df[FEATURES])[0]\n",
    "        pred_loss = pipeline_loss.predict(current_day_df[FEATURES])[0]\n",
    "        pred_entry = pipeline_entry.predict(current_day_df[FEATURES])[0]\n",
    "        tp_hit_prob = pipeline_class.predict_proba(current_day_df[FEATURES])[0][1]\n",
    "\n",
    "        current_price = current_day_df['Close'].values[0] \n",
    "        date = current_day_df.index[0]\n",
    "\n",
    "        # Use the predicted entry price for trade calculations\n",
    "        entry_price_for_trade = pred_entry \n",
    "        \n",
    "        # Basic sanity check for predicted entry price (e.g., must be somewhat realistic)\n",
    "        today_low = current_day_df['Low'].values[0]\n",
    "        if not np.isnan(today_low) and (entry_price_for_trade < today_low * 0.8 or entry_price_for_trade > today_low * 1.2):\n",
    "             signals.append({\n",
    "                'Date': date,\n",
    "                'Signal_Generated': False,\n",
    "                'Result': \"Skipped (Unrealistic Predicted Entry)\"\n",
    "            })\n",
    "             continue # Skip unreasonable entry predictions\n",
    "        \n",
    "        # Calculate TP/SL levels based on the predicted entry price\n",
    "        # Ensure predicted_loss is a negative value for SL calculation\n",
    "        sl_price = entry_price_for_trade * (1 + min(pred_loss, -STOP_LOSS)) # Take the min of predicted loss and -STOP_LOSS (e.g., if pred_loss is -0.03 and STOP_LOSS is 0.04, then -0.04 is used)\n",
    "        tp_price = entry_price_for_trade * (1 + pred_return)\n",
    "\n",
    "        # Look into the actual future (relative to the current_date_idx) for trade outcome\n",
    "        future_window_data = df_full.iloc[current_date_idx + 1 : current_date_idx + 1 + FORWARD_DAYS].copy()\n",
    "        \n",
    "        # Check if we can actually get into the trade based on the current market price\n",
    "        # This simulates whether the predicted entry price was achievable\n",
    "        if current_price <= entry_price_for_trade * tolerance: \n",
    "            # Trade is entered\n",
    "            exit_price = np.nan\n",
    "            result = \"No Exit\"\n",
    "            holding_days = 0\n",
    "\n",
    "            if not future_window_data.empty:\n",
    "                # Find the first day TP or SL is hit\n",
    "                tp_hit_day_idx = None\n",
    "                sl_hit_day_idx = None\n",
    "\n",
    "                for k, (fut_date, fut_row) in enumerate(future_window_data.iterrows()):\n",
    "                    if fut_row['High'] >= tp_price:\n",
    "                        tp_hit_day_idx = k\n",
    "                    if fut_row['Low'] <= sl_price:\n",
    "                        sl_hit_day_idx = k\n",
    "                    \n",
    "                    if tp_hit_day_idx is not None or sl_hit_day_idx is not None:\n",
    "                        break # Exit loop once either hit is detected\n",
    "\n",
    "                if tp_hit_day_idx is not None and (sl_hit_day_idx is None or tp_hit_day_idx <= sl_hit_day_idx):\n",
    "                    exit_price = tp_price \n",
    "                    result = \"TP Hit\"\n",
    "                    holding_days = tp_hit_day_idx + 1\n",
    "                elif sl_hit_day_idx is not None:\n",
    "                    exit_price = sl_price\n",
    "                    result = \"SL Hit\"\n",
    "                    holding_days = sl_hit_day_idx + 1\n",
    "                else: # No TP or SL hit within the window\n",
    "                    exit_price = future_window_data['Close'].iloc[-1]\n",
    "                    result = \"EOD Exit\"\n",
    "                    holding_days = FORWARD_DAYS\n",
    "            else: # If future_window_data is empty for some reason\n",
    "                exit_price = current_price # No exit, effectively\n",
    "                result = \"No Future Data\"\n",
    "                holding_days = 0\n",
    "\n",
    "            trade_return = (exit_price - entry_price_for_trade) / entry_price_for_trade if not pd.isna(exit_price) else np.nan\n",
    "\n",
    "            signals.append({\n",
    "                'Date': date,\n",
    "                'Price_at_Signal': current_price,\n",
    "                'Predicted_Entry': entry_price_for_trade,\n",
    "                'TP': tp_price,\n",
    "                'SL': sl_price,\n",
    "                'Exit': exit_price,\n",
    "                'Result': result,\n",
    "                'Holding_Days': holding_days,\n",
    "                'Return': trade_return,\n",
    "                'Signal_Generated': True,\n",
    "                'Predicted_Return': pred_return, # For internal check\n",
    "                'Predicted_Loss': pred_loss,     # For internal check\n",
    "                'TP_Hit_Prob': tp_hit_prob       # For internal check\n",
    "            })\n",
    "        else:\n",
    "            # Signal generated, but trade not taken due to price not reaching predicted entry within tolerance\n",
    "            signals.append({\n",
    "                'Date': date,\n",
    "                'Price_at_Signal': current_price,\n",
    "                'Predicted_Entry': entry_price_for_trade,\n",
    "                'TP': tp_price,\n",
    "                'SL': sl_price,\n",
    "                'Exit': np.nan,\n",
    "                'Result': \"No Trade (Entry Price Too High)\",\n",
    "                'Holding_Days': np.nan,\n",
    "                'Return': np.nan,\n",
    "                'Signal_Generated': False,\n",
    "                'Predicted_Return': pred_return,\n",
    "                'Predicted_Loss': pred_loss,\n",
    "                'TP_Hit_Prob': tp_hit_prob\n",
    "            })\n",
    "\n",
    "    # Convert signals to DataFrame and filter for actual trades for statistics\n",
    "    signals_df = pd.DataFrame(signals).set_index('Date')\n",
    "    trades = signals_df[signals_df['Signal_Generated']].copy()\n",
    "    trades = trades[trades['Return'].notna()] # Ensure return is calculated\n",
    "\n",
    "    if len(trades) == 0:\n",
    "        print(f\"[{ticker}] No valid trades generated after backtesting and filtering.\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "    # 4. Calculate Statistics\n",
    "    successful = trades[trades['Return'] > 0]\n",
    "    failed = trades[trades['Return'] <= 0]\n",
    "    \n",
    "    # Avoid division by zero for metrics like Win Rate, Avg Win/Loss, Profit Factor\n",
    "    total_trades = len(trades)\n",
    "    num_successful = len(successful)\n",
    "    num_failed = len(failed)\n",
    "\n",
    "    win_rate = num_successful / total_trades if total_trades > 0 else 0\n",
    "    avg_return = trades['Return'].mean()\n",
    "    avg_win = successful['Return'].mean() if num_successful > 0 else 0\n",
    "    avg_loss = failed['Return'].mean() if num_failed > 0 else 0 # Avg Loss should be negative\n",
    "    \n",
    "    # Median return/win/loss can be zero if no such trades occurred\n",
    "    median_return = trades['Return'].median()\n",
    "    median_win = successful['Return'].median() if num_successful > 0 else 0\n",
    "    median_loss = failed['Return'].median() if num_failed > 0 else 0\n",
    "\n",
    "    max_gain = trades['Return'].max()\n",
    "    max_loss = trades['Return'].min()\n",
    "    \n",
    "    avg_holding_days = trades['Holding_Days'].mean()\n",
    "\n",
    "    tp_hit_rate = (trades['Result'] == 'TP Hit').mean()\n",
    "    sl_hit_rate = (trades['Result'] == 'SL Hit').mean()\n",
    "    \n",
    "    # Return/Risk should be positive, so abs(avg_loss)\n",
    "    return_risk = -avg_win / avg_loss if avg_loss < 0 else (float('inf') if avg_win > 0 else 0)\n",
    "    \n",
    "    # Profit Factor\n",
    "    total_gross_profit = successful['Return'].sum()\n",
    "    total_gross_loss = abs(failed['Return'].sum()) # Sum of absolute losses\n",
    "    profit_factor = total_gross_profit / total_gross_loss if total_gross_loss > 0 else float('inf')\n",
    "\n",
    "    stats = {\n",
    "        'Total Trades': total_trades,\n",
    "        'Win Rate': win_rate,\n",
    "        'Avg Return': avg_return,\n",
    "        'Median Return': median_return,\n",
    "        'Avg Win': avg_win,\n",
    "        'Median Win': median_win,\n",
    "        'Max Gain': max_gain,\n",
    "        'Avg Loss': avg_loss,\n",
    "        'Median Loss': median_loss,\n",
    "        'Max Loss': max_loss,\n",
    "        'Avg Holding Days': avg_holding_days,\n",
    "        'TP Hit Rate': tp_hit_rate,\n",
    "        'SL Hit Rate': sl_hit_rate,\n",
    "        'Return/Risk': return_risk,\n",
    "        'Profit Factor': profit_factor\n",
    "    }\n",
    "\n",
    "    # 5. Enhanced Visualization\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    grid = plt.GridSpec(4, 1, height_ratios=[3, 1, 1, 1])\n",
    "    \n",
    "    # Price Chart\n",
    "    ax1 = plt.subplot(grid[0])\n",
    "    # Plot the full dataframe data (including training portion)\n",
    "    plt.plot(df_full.index, df_full['Close'], label=ticker, color='grey', alpha=0.6)\n",
    "    plt.plot(df_full.index, df_full['SMA1'], label='MA12', color='orange', alpha=0.6)\n",
    "    plt.plot(df_full.index, df_full['SMA2'], label='MA24', color='red', alpha=0.6)\n",
    "    \n",
    "    plt.fill_between(df_full.index, df_full['SMA1'], df_full['SMA2'],\n",
    "                     where=(df_full['SMA1'] > df_full['SMA2']), interpolate=True,\n",
    "                     color='limegreen', alpha=0.2, label='Bullish', zorder=0)\n",
    "    \n",
    "    plt.fill_between(df_full.index, df_full['SMA1'], df_full['SMA2'],\n",
    "                     where=(df_full['SMA1'] < df_full['SMA2']), interpolate=True,\n",
    "                     color='tomato', alpha=0.2, label='Bearish', zorder=0)\n",
    "    \n",
    "    # Plot actual trades (filtered for Signal_Generated=True)\n",
    "    sample_trades_to_plot = trades.iloc[::show_every_n] # Sample to avoid clutter\n",
    "    \n",
    "    # Entry points (Predicted_Entry is the target price)\n",
    "    plt.scatter(sample_trades_to_plot.index, sample_trades_to_plot['Predicted_Entry'], color='green', \n",
    "                marker='^', s=50, label='Predicted Entry', zorder=3, alpha = 0.7)\n",
    "    \n",
    "    # Exit points (color by result)\n",
    "    colors = {'TP Hit':'blue', 'SL Hit':'red', 'EOD Exit':'gray'}\n",
    "    for res_type, group in sample_trades_to_plot.groupby('Result'):\n",
    "        if res_type in colors: # Only plot known result types\n",
    "            plt.scatter(group.index, group['Exit'], \n",
    "                        color=colors[res_type], marker='o', \n",
    "                        s=50, label=f'{res_type}', zorder=2, alpha = 0.7)\n",
    "    \n",
    "    # Annotations for sampled trades\n",
    "    for date, row in sample_trades_to_plot.iterrows():\n",
    "        # Only annotate if Exit price is not NaN\n",
    "        if not pd.isna(row['Exit']):\n",
    "            if row['Return'] > 0:  # Winning trade\n",
    "                plt.annotate(f\"TP: {row['TP']:.1f}\\n({row['Return']:.1%})\",\n",
    "                             (date, row['TP']), \n",
    "                             xytext=(0,10), textcoords='offset points', ha='center', \n",
    "                             va='bottom', color='green', fontsize=8,\n",
    "                             bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "            else:  # Losing trade\n",
    "                plt.annotate(f\"SL: {row['SL']:.1f}\\n({row['Return']:.1%})\",\n",
    "                             (date, row['SL']), \n",
    "                             xytext=(0,-10), textcoords='offset points', ha='center', \n",
    "                             va='top', color='red', fontsize=8,\n",
    "                             bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "\n",
    "    ax1.text(0.5, 0.5, f'{ticker}', transform=ax1.transAxes, \n",
    "             fontsize=50, color='grey', alpha=0.2,\n",
    "             horizontalalignment='center', verticalalignment='center',\n",
    "             rotation=0, weight='bold', style='italic')\n",
    "    \n",
    "    plt.title(f\"{ticker} Backtest Results | {df_full.index[start_idx_for_backtest_loop].date()} to {df_full.index[-1].date()}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add RSI with curved fill\n",
    "    axRSI = plt.subplot(grid[1])\n",
    "    plt.plot(df_full.index, df_full['RSI'], label='RSI', color='grey', alpha=0.6, zorder=2)\n",
    "    \n",
    "    plt.fill_between(df_full.index, df_full['RSI'], 50, where=(df_full['RSI'] >= 50),\n",
    "                     interpolate=True, color='limegreen', alpha=0.2, zorder=1)\n",
    "    \n",
    "    plt.fill_between(df_full.index, df_full['RSI'], 50, where=(df_full['RSI'] <= 50),\n",
    "                     interpolate=True, color='tomato', alpha=0.2, zorder=1)\n",
    "    \n",
    "    plt.axhline(50, color='red', linestyle='-', alpha=0.2, zorder=0)\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.ylabel('RSI')\n",
    "            \n",
    "    # Returns Distribution\n",
    "    ax2 = plt.subplot(grid[2])\n",
    "    plt.hist(trades['Return'], bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(x=0, color='red', linestyle='--')\n",
    "    plt.title('Returns Distribution')\n",
    "    plt.xlabel(f'{FORWARD_DAYS}-day Return')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Cumulative Returns\n",
    "    CAPITAL = 1000\n",
    "    ax3 = plt.subplot(grid[3])\n",
    "    # Ensure trades are sorted by date for cumulative returns\n",
    "    cumulative_returns = (1 + trades.sort_index()['Return']).cumprod()\n",
    "    (cumulative_returns * CAPITAL).plot(color='green', label='Strategy')\n",
    "    plt.axhline(y=CAPITAL, color='red', linestyle='--')\n",
    "    plt.title('Cumulative Returns')\n",
    "    plt.ylabel(f'Growth of ${CAPITAL}')\n",
    "    plt.grid(True)\n",
    "    plt.yscale(\"log\")  # Use log scale for better visualization of compounded returns\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Print Statistics\n",
    "    print(f\"\\n=== {ticker} Backtest Results ===\")\n",
    "    if not trades.empty:\n",
    "        print(f\"Backtest Period: {trades.index[0].date()} to {trades.index[-1].date()}\")\n",
    "    else:\n",
    "        print(\"No trades executed in the backtest period.\")\n",
    "    print(f\"Total Trades: {stats['Total Trades']}\")\n",
    "    print(f\"Win Rate: {stats['Win Rate']:.1%}\")\n",
    "    print(f\"Avg Return: {stats['Avg Return']:.2%}\")\n",
    "    print(f\"Avg Win: {stats['Avg Win']:.2%}\")\n",
    "    print(f\"Avg Loss: {stats['Avg Loss']:.2%}\")\n",
    "    print(f\"Median Return: {stats['Median Return']:.2%}\")\n",
    "    print(f\"Avg Holding Days: {stats['Avg Holding Days']:.1f}\")\n",
    "    print(f\"TP Hit Rate: {stats['TP Hit Rate']:.1%}\")\n",
    "    print(f\"SL Hit Rate: {stats['SL Hit Rate']:.1%}\")\n",
    "    print(f\"Max Gain: {stats['Max Gain']:.2%}\")\n",
    "    print(f\"Max Loss: {stats['Max Loss']:.2%}\")\n",
    "    print(f\"Return/Risk Ratio: {stats['Return/Risk']:.2f}\")\n",
    "    print(f\"Profit Factor: {stats['Profit Factor']:.2f}\")\n",
    "    print(f\"\\nAvg Win/Loss {stats['Avg Win']:.2%} to {stats['Avg Loss']:.2%}\")\n",
    "    return trades, stats\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "\n",
    "# 1. Run Live Predictions\n",
    "print(\"--- Running Live Predictions ---\")\n",
    "live_predictions_df = make_live_predictions(TICKERS, start_date, end_date, FEATURES)\n",
    "\n",
    "print(\"\\n=== Live Predictions Summary ===\")\n",
    "if not live_predictions_df.empty:\n",
    "    print(tabulate(live_predictions_df.set_index('Ticker'), headers='keys', tablefmt='psql', floatfmt=\".2f\"))\n",
    "\n",
    "    # Plot Live Predictions\n",
    "    df_plot = live_predictions_df.copy()\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6), dpi=100)\n",
    "\n",
    "    sns.barplot(x=\"Ticker\", y=\"Predicted_Max_Return_Pct\", data=df_plot, palette=\"Spectral\", ax=ax1)\n",
    "    ax1.set_ylabel('Predicted Max Return (%)', fontsize=12)\n",
    "    ax1.set_xlabel('Ticker', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(x=\"Ticker\", y=\"Predicted_Max_Loss_Pct\", data=df_plot, color='red', marker='o', \n",
    "                 ax=ax2, linewidth=2, markersize=8, label='Predicted Max Loss')\n",
    "    ax2.set_ylabel('Predicted Max Loss (%)', fontsize=12, color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    for i, (_, row) in enumerate(df_plot.iterrows()):\n",
    "        # Values for plotting (ensure they are numerical)\n",
    "        max_return_val = row[\"Predicted_Max_Return_Pct\"]\n",
    "        loss_val = row[\"Predicted_Max_Loss_Pct\"]\n",
    "        \n",
    "        fcolor = 'green' if row.Technical_Signal == \"âœ… Bullish\" else ('red' if row.Technical_Signal == \"ðŸ”» Bearish\" else 'yellow')\n",
    "        prob_color = 'green' if (row.Confidence_Score > 40 and row.TP_Hit_Probability > 40) else 'white'\n",
    "                \n",
    "        ax1.text(i, max_return_val + 0.5, f'{max_return_val:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "        ax2.text(i, loss_val - 0.5, f'{loss_val:.1f}%', ha='center', va='top', color='red', fontsize=9)\n",
    "        \n",
    "        vertical_offset = 5 # Base offset in percentage points\n",
    "        off1 = vertical_offset + 3 # Additional space for Entry box\n",
    "        off2 = vertical_offset # TP/SL at base offset\n",
    "\n",
    "        ax1.text(i, -off2,\n",
    "                 f'Cur: ${row[\"Current_Price\"]:.1f}\\nPred Entry: ${row[\"Predicted_Entry\"]:.1f}\\n{row[\"Entry_Diff_Pct\"]:.1f}%\\n{row[\"Technical_Signal\"]}',\n",
    "                 ha='center', va='top', fontsize=8, \n",
    "                 bbox=dict(facecolor=fcolor, alpha=0.1, linewidth=0.3))\n",
    "        \n",
    "        ax1.text(i, -off1,\n",
    "                 f'TP: ${row[\"Predicted_TP_Price\"]:.1f}\\nSL: ${row[\"Predicted_SL_Price\"]:.1f}\\n\\nProb: {row[\"TP_Hit_Probability\"]:.0f}\\nConf: {row[\"Confidence_Score\"]:.0f}', \n",
    "                 ha='center', va='top', fontsize=8, \n",
    "                 bbox=dict(facecolor=prob_color, alpha=0.1, linewidth=0.3))\n",
    "\n",
    "    textbox = AnchoredText(\n",
    "        \"Hint: Buy closer to predicted Entry price for better risk-reward.\",\n",
    "        loc='lower left', frameon=True, borderpad=1.5,\n",
    "        prop=dict(size=10, color='gray', weight='bold')\n",
    "    )\n",
    "    ax1.add_artist(textbox)\n",
    "    textbox.patch.set_facecolor('honeydew')\n",
    "    textbox.patch.set_edgecolor('darkgreen')\n",
    "    textbox.patch.set_alpha(0.8)\n",
    "\n",
    "    plt.title(\"ML Predictions (Return/Loss %)\", fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No live predictions generated.\")\n",
    "\n",
    "\n",
    "# 2. Run Backtests\n",
    "print(\"\\n--- Running Backtests ---\")\n",
    "all_stats = []\n",
    "\n",
    "for tick in TICKERS:\n",
    "    # Use the revised backtesting function\n",
    "    trades, stats = train_and_backtest_revised(tick, initial_train_years=2, backtest_days=252*1, show_every_n=10) # show every 10th trade on plot\n",
    "    \n",
    "    if trades.empty:\n",
    "        print(f\"{tick} has no trades after filtering in backtest.\\n\")\n",
    "        continue\n",
    "    \n",
    "    stats_row = {\n",
    "        'Ticker': tick,\n",
    "        'Total Trades': stats['Total Trades'], # Added total trades to summary\n",
    "        'Win Rate': stats['Win Rate'],\n",
    "        'TP Hit Rate': stats['TP Hit Rate'],\n",
    "        'SL Hit Rate': stats['SL Hit Rate'],\n",
    "        'Avg Return': stats['Avg Return'],\n",
    "        'Avg Win': stats['Avg Win'],\n",
    "        'Avg Loss': stats['Avg Loss'],\n",
    "        'Profit Factor': stats['Profit Factor'],\n",
    "        'Avg Holding Days': stats['Avg Holding Days']\n",
    "    }\n",
    "    all_stats.append(stats_row)\n",
    "\n",
    "summary_df = pd.DataFrame(all_stats)\n",
    "summary_df = summary_df.sort_values(by='Profit Factor', ascending=False)\n",
    "\n",
    "# Format for display\n",
    "summary_df['Win Rate'] = (summary_df['Win Rate'] * 100).round(1).astype(str) + '%'\n",
    "summary_df['TP Hit Rate'] = (summary_df['TP Hit Rate'] * 100).round(1).astype(str) + '%'\n",
    "summary_df['SL Hit Rate'] = (summary_df['SL Hit Rate'] * 100).round(1).astype(str) + '%'\n",
    "summary_df['Avg Return'] = (summary_df['Avg Return'] * 100).round(2).astype(str) + '%'\n",
    "summary_df['Avg Win'] = (summary_df['Avg Win'] * 100).round(2).astype(str) + '%'\n",
    "summary_df['Avg Loss'] = (summary_df['Avg Loss'] * 100).round(2).astype(str) + '%'\n",
    "summary_df['Profit Factor'] = summary_df['Profit Factor'].round(2).astype(str)\n",
    "summary_df['Avg Holding Days'] = summary_df['Avg Holding Days'].round(1).astype(str)\n",
    "\n",
    "print(\"\\n=== Ticker Backtest Stats Summary ===\")\n",
    "print(tabulate(summary_df, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b0830-cba7-4f29-a303-3cbe936d6f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
